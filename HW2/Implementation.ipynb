{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import lowess, logistic, gradientboost\n",
    "from GradientBooster import GradientBooster\n",
    "from Lowess import Lowess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# !pip install xgboost\n",
    "import xgboost\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,QuantileTransformer\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Kernel\n",
    "def Gaussian(x):\n",
    "  return np.where(np.abs(x)>4,0,1/(np.sqrt(2*np.pi))*np.exp(-1/2*x**2))\n",
    "    \n",
    "# Tricubic Kernel\n",
    "def Tricubic(x):\n",
    "  return np.where(np.abs(x)>1,0,(1-np.abs(x)**3)**3)\n",
    "    \n",
    "# Epanechnikov Kernel\n",
    "def Epanechnikov(x):\n",
    "  return np.where(np.abs(x)>1,0,3/4*(1-np.abs(x)**2))\n",
    "    \n",
    "# Quartic Kernel\n",
    "def Quartic(x):\n",
    "  return np.where(np.abs(x)>1,0,15/16*(1-np.abs(x)**2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = Gaussian\n",
    "tau = 0.5  # You may need to tune this\n",
    "\n",
    "lowess_model = Lowess(kernel, tau)\n",
    "gb_model = GradientBooster(lowess_model, Lowess(kernel, tau), n_boosting_steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/concrete.csv')\n",
    "\n",
    "X = data.drop(columns='strength').values\n",
    "y = data['strength'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(model):\n",
    "    results = {}\n",
    "\n",
    "    # list of scalers to iterate through\n",
    "    scalers = {\n",
    "    \"StandardScaler\": StandardScaler(),\n",
    "    \"MinMaxScaler\": MinMaxScaler(),\n",
    "    \"QuantileScaler\": QuantileTransformer(n_quantiles=min(100, X.shape[0]), output_distribution='normal')\n",
    "    }\n",
    "\n",
    "    # initializing kfold cross-validation\n",
    "    kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    for scaler_name, scaler in scalers.items():\n",
    "        # scaling features using the current scaler\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        # making list to store mse for each fold\n",
    "        mse_scores = []\n",
    "\n",
    "   \n",
    "        for train_index, test_index in kf.split(X_scaled):\n",
    "            # splitting into testing and training data\n",
    "            X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            # Clone the model for each fold\n",
    "            model_clone = clone(model)\n",
    "            \n",
    "            # Fit the cloned model\n",
    "            model_clone.fit(X_train, y_train)\n",
    "\n",
    "            # Predict on test data\n",
    "            y_pred = model_clone.predict(X_test)\n",
    "            \n",
    "            # calculate and append mse\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mse_scores.append(mse)\n",
    "\n",
    "        # calculate the mean of the mse for each scaler\n",
    "        results[scaler_name] = np.mean(mse_scores)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = tts(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Model is not fitted. Please call 'fit' before predicting.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gb_results \u001b[38;5;241m=\u001b[39m k_fold_cross_validation(gb_model)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradientBooster results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scaler, mse \u001b[38;5;129;01min\u001b[39;00m gb_results\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m, in \u001b[0;36mk_fold_cross_validation\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     28\u001b[0m model_clone \u001b[38;5;241m=\u001b[39m clone(model)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Fit the cloned model\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m model_clone\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Predict on test data\u001b[39;00m\n\u001b[1;32m     34\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model_clone\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/Documents/WM/Fall_24/DATA400/HW2/GradientBooster.py:33\u001b[0m, in \u001b[0;36mGradientBooster.fit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03mFit the GradientBooster to the training data.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m    y_train: Training target values (numpy array or series)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(X_train, y_train) \u001b[38;5;66;03m# Fit the main model to the training data\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(X_train) \u001b[38;5;66;03m# Get the initial predictions from the main model\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_boosting_steps): \u001b[38;5;66;03m# Iteratively fit residuals with boosting steps\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Calculate residuals (difference between actual target and predicted)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     residuals \u001b[38;5;241m=\u001b[39m y_train \u001b[38;5;241m-\u001b[39m y_pred\n",
      "File \u001b[0;32m~/Documents/WM/Fall_24/DATA400/HW2/GradientBooster.py:63\u001b[0m, in \u001b[0;36mGradientBooster.predict\u001b[0;34m(self, X_test)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mPredict target values for the test data using the fitted GradientBooster.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03m    Final predictions after boosting (numpy array)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fitted:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel is not fitted. Please call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m before predicting.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;66;03m# Get initial predictions from the main model\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res_model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresiduals:\n",
      "\u001b[0;31mValueError\u001b[0m: Model is not fitted. Please call 'fit' before predicting."
     ]
    }
   ],
   "source": [
    "gb_results = k_fold_cross_validation(gb_model)\n",
    "print(\"GradientBooster results:\")\n",
    "for scaler, mse in gb_results.items():\n",
    "    print(f\"{scaler}: MSE = {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
